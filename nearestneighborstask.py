# -*- coding: utf-8 -*-
"""NearestNeighborsTask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18VDxAACvnEBK1UcECbl7dKtfR0FKuGOa
"""

import numpy as np

# Dataset with features and labels
fruit_data = [
    [150, 7.0, 1, 'Apple'],
    [120, 6.5, 0, 'Banana'],
    [180, 7.5, 2, 'Orange'],
    [155, 7.2, 1, 'Apple'],
    [110, 6.0, 0, 'Banana'],
    [190, 7.8, 2, 'Orange'],
    [145, 7.1, 1, 'Apple'],
    [115, 6.3, 0, 'Banana']
]

# Extract features (X) and labels (y)
labels = [entry[3] for entry in fruit_data]
features = [entry[:-1] for entry in fruit_data]

# Label encoding
label_map = {'Apple': 1, 'Banana': 0, 'Orange': 2}
encoded_labels = [label_map.get(label) for label in labels]

print("Encoded Labels:", encoded_labels)
print("Features:", features)

# Convert to numpy arrays
features_array = np.array(features)
features_2d = features_array[:, [0, 1]]  # Select weight and size

# Test dataset
test_features_2d = np.array([
    [118, 6.2],  # Banana
    [160, 7.3],  # Apple
    [185, 7.7]   # Orange
])

# Define distance metric
def calculate_euclidean_distance(point1, point2):
    total = 0
    for i in range(len(point1)):
        total += (point1[i] - point2[i]) ** 2
    return total ** 0.5

# KNN Classifier
class KNearestNeighbors:
    def __init__(self, neighbors=3):
        self.neighbors = neighbors
        self.training_features = None
        self.training_labels = None

    def train(self, X, y):
        self.training_features = np.array(X)
        self.training_labels = np.array(y)

    def predict_single(self, sample):
        distances = np.sqrt(np.sum((self.training_features - sample) ** 2, axis=1))
        nearest_indices = np.argsort(distances)[:self.neighbors]
        nearest_labels = self.training_labels[nearest_indices]
        unique, counts = np.unique(nearest_labels, return_counts=True)
        return unique[np.argmax(counts)]

    def predict(self, X_test):
        X_test = np.array(X_test)
        return np.array([self.predict_single(x) for x in X_test])

# Run KNN on training data
knn_model = KNearestNeighbors(neighbors=3)
knn_model.train(features_array, encoded_labels)
train_predictions = knn_model.predict(features_array)
print("Training Predictions:", train_predictions)

# Test KNN on test dataset
test_features = np.array([
    [118, 6.2, 0],  # Banana
    [160, 7.3, 1],  # Apple
    [185, 7.7, 2]   # Orange
])
test_predictions = knn_model.predict(test_features)
print("Test Predictions:", test_predictions)

# Evaluate different k values
k_options = [1, 3, 5]
expected_results = [0, 1, 2]
for k in k_options:
    knn_model = KNearestNeighbors(neighbors=k)
    knn_model.train(features_array, encoded_labels)
    predictions = knn_model.predict(test_features)
    print(f"Predictions for k={k}: {predictions}")
    accuracy = np.mean(predictions == expected_results)
    print(f"Accuracy for k={k}: {accuracy}")

# Feature Normalization
def min_max_scaling(data):
    data = np.array(data)
    min_values = data.min(axis=0)
    max_values = data.max(axis=0)
    return (data - min_values) / (max_values - min_values)

# Apply min-max normalization
knn_normalized = KNearestNeighbors(neighbors=3)
knn_normalized.train(min_max_scaling(features_array), encoded_labels)
normalized_predictions = knn_normalized.predict(min_max_scaling(test_features))
print("Min-Max Normalized Predictions:", normalized_predictions)

def z_score_scaling(data):
    data = np.array(data)
    mean_values = data.mean(axis=0)
    std_values = data.std(axis=0)
    return (data - mean_values) / std_values

# Apply z-score normalization
knn_zscore = KNearestNeighbors(neighbors=3)
knn_zscore.train(z_score_scaling(features_array), encoded_labels)
zscore_predictions = knn_zscore.predict(z_score_scaling(test_features))
print("Z-Score Normalized Predictions:", zscore_predictions)

# Manhattan Distance KNN
class KNN_ManhattanDistance:
    def __init__(self, neighbors=3):
        self.neighbors = neighbors
        self.training_features = None
        self.training_labels = None

    def train(self, X, y):
        self.training_features = np.array(X)
        self.training_labels = np.array(y)

    def predict_single(self, sample):
        distances = np.sum(np.abs(self.training_features - sample), axis=1)
        nearest_indices = np.argsort(distances)[:self.neighbors]
        nearest_labels = self.training_labels[nearest_indices]
        unique, counts = np.unique(nearest_labels, return_counts=True)
        return unique[np.argmax(counts)]

    def predict(self, X_test):
        X_test = np.array(X_test)
        return np.array([self.predict_single(x) for x in X_test])

knn_manhattan = KNN_ManhattanDistance(neighbors=3)
knn_manhattan.train(features_array, encoded_labels)
manhattan_predictions = knn_manhattan.predict(test_features)
print("Manhattan Predictions:", manhattan_predictions)

# Minkowski Distance KNN
class KNN_MinkowskiDistance:
    def __init__(self, power, neighbors=3):
        self.neighbors = neighbors
        self.training_features = None
        self.training_labels = None
        self.power = power

    def train(self, X, y):
        self.training_features = np.array(X)
        self.training_labels = np.array(y)

    def predict_single(self, sample):
        distances = np.sum(np.abs(self.training_features - sample) ** self.power, axis=1) ** (1 / self.power)
        nearest_indices = np.argsort(distances)[:self.neighbors]
        nearest_labels = self.training_labels[nearest_indices]
        unique, counts = np.unique(nearest_labels, return_counts=True)
        return unique[np.argmax(counts)]

    def predict(self, X_test):
        X_test = np.array(X_test)
        return np.array([self.predict_single(x) for x in X_test])

knn_minkowski = KNN_MinkowskiDistance(neighbors=3, power=4)
knn_minkowski.train(features_array, encoded_labels)
minkowski_predictions = knn_minkowski.predict(test_features)
print("Minkowski Predictions:", minkowski_predictions)

# Visualization
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

class KNN_Visualizer:
    def __init__(self, neighbors=3, metric='euclidean', power=3):
        self.neighbors = neighbors
        self.metric = metric
        self.power = power

    def train(self, X, y):
        self.training_features = np.array(X)
        self.training_labels = np.array(y)

    def compute_distances(self, sample):
        if self.metric == 'euclidean':
            return np.sqrt(np.sum((self.training_features - sample) ** 2, axis=1))
        elif self.metric == 'manhattan':
            return np.sum(np.abs(self.training_features - sample), axis=1)
        elif self.metric == 'minkowski':
            return np.sum(np.abs(self.training_features - sample) ** self.power, axis=1) ** (1 / self.power)
        else:
            raise ValueError("Invalid metric")

    def predict_single(self, sample):
        distances = self.compute_distances(sample)
        nearest_indices = np.argsort(distances)[:self.neighbors]
        nearest_labels = self.training_labels[nearest_indices]
        unique, counts = np.unique(nearest_labels, return_counts=True)
        return unique[np.argmax(counts)]

    def predict(self, X_test):
        return np.array([self.predict_single(x) for x in X_test])

# Normalize features for visualization
normalized_features = min_max_scaling(features_2d)

# Create grid
step_size = 0.01
x_lower, x_upper = normalized_features[:, 0].min() - 0.1, normalized_features[:, 0].max() + 0.1
y_lower, y_upper = normalized_features[:, 1].min() - 0.1, normalized_features[:, 1].max() + 0.1
grid_x, grid_y = np.meshgrid(np.arange(x_lower, x_upper, step_size),
                             np.arange(y_lower, y_upper, step_size))
grid_samples = np.c_[grid_x.ravel(), grid_y.ravel()]

def visualize_boundaries(neighbors=3, metric='euclidean', power=3):
    knn_vis = KNN_Visualizer(neighbors=neighbors, metric=metric, power=power)
    knn_vis.train(normalized_features, encoded_labels)
    boundary_predictions = knn_vis.predict(grid_samples)
    boundary_predictions = boundary_predictions.reshape(grid_x.shape)

    light_cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    bold_colors = ['red', 'green', 'blue']

    plt.figure(figsize=(6, 5))
    plt.contourf(grid_x, grid_y, boundary_predictions, cmap=light_cmap, alpha=0.7)
    for i, color in zip(range(3), bold_colors):
        plt.scatter(normalized_features[encoded_labels == i][:, 0],
                    normalized_features[encoded_labels == i][:, 1],
                    label=f'Class {i}', color=color, edgecolor='k', s=60)
    plt.title(f"Decision Boundary (k={neighbors}, metric={metric})")
    plt.xlabel("Normalized Weight")
    plt.ylabel("Normalized Size")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Visualize different metrics
visualize_boundaries(neighbors=3, metric='euclidean')
visualize_boundaries(neighbors=3, metric='manhattan')
visualize_boundaries(neighbors=3, metric='minkowski', power=3)

# Weighted KNN
class WeightedKNNClassifier:
    def __init__(self, neighbors=3, metric='euclidean', power=3):
        self.neighbors = neighbors
        self.metric = metric
        self.power = power

    def train(self, X, y):
        self.training_features = np.array(X)
        self.training_labels = np.array(y)

    def compute_distances(self, sample):
        if self.metric == 'euclidean':
            return np.sqrt(np.sum((self.training_features - sample) ** 2, axis=1))
        elif self.metric == 'manhattan':
            return np.sum(np.abs(self.training_features - sample), axis=1)
        elif self.metric == 'minkowski':
            return np.sum(np.abs(self.training_features - sample) ** self.power, axis=1) ** (1 / self.power)
        else:
            raise ValueError("Invalid metric")

    def predict_single(self, sample):
        distances = self.compute_distances(sample)
        epsilon = 1e-10
        nearest_indices = np.argsort(distances)[:self.neighbors]
        nearest_distances = distances[nearest_indices]
        nearest_labels = self.training_labels[nearest_indices]
        weights = 1 / (nearest_distances + epsilon)
        unique_labels = np.unique(self.training_labels)
        vote_scores = np.zeros(len(unique_labels))
        for i, label in enumerate(unique_labels):
            label_mask = nearest_labels == label
            vote_scores[i] = np.sum(weights[label_mask])
        return unique_labels[np.argmax(vote_scores)]

    def predict(self, X_test):
        return np.array([self.predict_single(x) for x in X_test])

weighted_knn = WeightedKNNClassifier(neighbors=3)
weighted_knn.train(features_2d, encoded_labels)
weighted_predictions = weighted_knn.predict(test_features_2d)
print("Weighted KNN Predictions:", weighted_predictions)

# Visualize Weighted KNN
def visualize_weighted_boundaries(neighbors=3, metric='euclidean', power=3):
    weighted_knn = WeightedKNNClassifier(neighbors=neighbors, metric=metric, power=power)
    weighted_knn.train(normalized_features, encoded_labels)
    boundary_predictions = weighted_knn.predict(grid_samples)
    boundary_predictions = boundary_predictions.reshape(grid_x.shape)

    light_cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    bold_colors = ['red', 'green', 'blue']

    plt.figure(figsize=(6, 5))
    plt.contourf(grid_x, grid_y, boundary_predictions, cmap=light_cmap, alpha=0.7)
    for i, color in zip(range(3), bold_colors):
        plt.scatter(normalized_features[encoded_labels == i][:, 0],
                    normalized_features[encoded_labels == i][:, 1],
                    label=f'Class {i}', color=color, edgecolor='k', s=60)
    plt.title(f"Weighted Decision Boundary (k={neighbors}, metric={metric})")
    plt.xlabel("Normalized Weight")
    plt.ylabel("Normalized Size")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Visualize weighted boundaries
visualize_weighted_boundaries(neighbors=3, metric='euclidean')
visualize_weighted_boundaries(neighbors=3, metric='manhattan')
visualize_weighted_boundaries(neighbors=3, metric='minkowski', power=3)

